{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae444728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c41137a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_train\n",
    "X_train = X_train\n",
    "\n",
    "%store -r X_val\n",
    "X_val = X_val\n",
    "\n",
    "%store -r y_train\n",
    "y_train = y_train\n",
    "\n",
    "%store -r y_val\n",
    "y_val = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f6f75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1c20fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üîπ Logistic Regression\n",
      "Test Accuracy: 0.7654\n",
      "Training Time: 0.0405 sec\n",
      "Prediction Time: 0.000728 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.74      0.76      1424\n",
      "        True       0.75      0.79      0.77      1445\n",
      "\n",
      "    accuracy                           0.77      2869\n",
      "   macro avg       0.77      0.77      0.77      2869\n",
      "weighted avg       0.77      0.77      0.77      2869\n",
      "\n",
      "==================================================\n",
      "üîπ KNN\n",
      "Test Accuracy: 0.7654\n",
      "Training Time: 0.0020 sec\n",
      "Prediction Time: 3.611320 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.78      0.77      1424\n",
      "        True       0.77      0.75      0.76      1445\n",
      "\n",
      "    accuracy                           0.77      2869\n",
      "   macro avg       0.77      0.77      0.77      2869\n",
      "weighted avg       0.77      0.77      0.77      2869\n",
      "\n",
      "==================================================\n",
      "üîπ SVC\n",
      "Test Accuracy: 0.7881\n",
      "Training Time: 9.9314 sec\n",
      "Prediction Time: 0.890653 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.75      0.78      1424\n",
      "        True       0.77      0.83      0.80      1445\n",
      "\n",
      "    accuracy                           0.79      2869\n",
      "   macro avg       0.79      0.79      0.79      2869\n",
      "weighted avg       0.79      0.79      0.79      2869\n",
      "\n",
      "==================================================\n",
      "üîπ Random Forest\n",
      "Test Accuracy: 0.7835\n",
      "Training Time: 1.1757 sec\n",
      "Prediction Time: 0.082987 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.80      0.79      1424\n",
      "        True       0.80      0.76      0.78      1445\n",
      "\n",
      "    accuracy                           0.78      2869\n",
      "   macro avg       0.78      0.78      0.78      2869\n",
      "weighted avg       0.78      0.78      0.78      2869\n",
      "\n",
      "==================================================\n",
      "üîπ Naive Bayes\n",
      "Test Accuracy: 0.7483\n",
      "Training Time: 0.0113 sec\n",
      "Prediction Time: 0.002666 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.79      0.76      1424\n",
      "        True       0.77      0.71      0.74      1445\n",
      "\n",
      "    accuracy                           0.75      2869\n",
      "   macro avg       0.75      0.75      0.75      2869\n",
      "weighted avg       0.75      0.75      0.75      2869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikolosova\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [15:26:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üîπ XGBoost\n",
      "Test Accuracy: 0.7975\n",
      "Training Time: 0.3929 sec\n",
      "Prediction Time: 0.003434 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.77      0.79      1424\n",
      "        True       0.78      0.82      0.80      1445\n",
      "\n",
      "    accuracy                           0.80      2869\n",
      "   macro avg       0.80      0.80      0.80      2869\n",
      "weighted avg       0.80      0.80      0.80      2869\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2933, number of negative: 2891\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 5824, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503606 -> initscore=0.014423\n",
      "[LightGBM] [Info] Start training from score 0.014423\n",
      "==================================================\n",
      "üîπ LightGBM\n",
      "Test Accuracy: 0.7961\n",
      "Training Time: 0.5373 sec\n",
      "Prediction Time: 0.008997 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.76      0.79      1424\n",
      "        True       0.78      0.83      0.80      1445\n",
      "\n",
      "    accuracy                           0.80      2869\n",
      "   macro avg       0.80      0.80      0.80      2869\n",
      "weighted avg       0.80      0.80      0.80      2869\n",
      "\n",
      "==================================================\n",
      "üîπ CatBoost\n",
      "Test Accuracy: 0.7999\n",
      "Training Time: 10.2494 sec\n",
      "Prediction Time: 0.014925 sec\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.76      0.79      1424\n",
      "        True       0.78      0.84      0.81      1445\n",
      "\n",
      "    accuracy                           0.80      2869\n",
      "   macro avg       0.80      0.80      0.80      2869\n",
      "weighted avg       0.80      0.80      0.80      2869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Train, predict, evaluate with timing\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Training time\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    # Prediction time\n",
    "    start_pred = time.time()\n",
    "    y_pred = model.predict(X_val)\n",
    "    end_pred = time.time()\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    train_time = end_train - start_train\n",
    "    pred_time = end_pred - start_pred\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Test Accuracy\": acc,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Prediction Time (s)\": pred_time\n",
    "    })\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"üîπ {name}\")\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Training Time: {train_time:.4f} sec\")\n",
    "    print(f\"Prediction Time: {pred_time:.6f} sec\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec5cfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Model Comparison:\n",
      "                  Model  Test Accuracy  Train Time (s)  Prediction Time (s)\n",
      "7             CatBoost       0.799930       10.249419             0.014925\n",
      "5              XGBoost       0.797490        0.392875             0.003434\n",
      "6             LightGBM       0.796096        0.537251             0.008997\n",
      "2                  SVC       0.788079        9.931385             0.890653\n",
      "3        Random Forest       0.783548        1.175706             0.082987\n",
      "1                  KNN       0.765423        0.001997             3.611320\n",
      "0  Logistic Regression       0.765423        0.040546             0.000728\n",
      "4          Naive Bayes       0.748344        0.011261             0.002666\n"
     ]
    }
   ],
   "source": [
    "# 4. Compare results in a table\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "print(\"\\nüìä Model Comparison:\\n\", results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0b2f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. –ö—Ä–æ—Å-–≤–∞–ª—ñ–¥–∞—Ü—ñ—è (StratifiedKFold –∑–±–µ—Ä—ñ–≥–∞—î –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# 3. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π\n",
    "models = {\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bad3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2640, number of negative: 2601\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1651\n",
      "[LightGBM] [Info] Number of data points in the train set: 5241, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503721 -> initscore=0.014883\n",
      "[LightGBM] [Info] Start training from score 0.014883\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2639, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 5241, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503530 -> initscore=0.014120\n",
      "[LightGBM] [Info] Start training from score 0.014120\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2639, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1652\n",
      "[LightGBM] [Info] Number of data points in the train set: 5241, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503530 -> initscore=0.014120\n",
      "[LightGBM] [Info] Start training from score 0.014120\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2639, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1652\n",
      "[LightGBM] [Info] Number of data points in the train set: 5241, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503530 -> initscore=0.014120\n",
      "[LightGBM] [Info] Start training from score 0.014120\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2640, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1652\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503625 -> initscore=0.014499\n",
      "[LightGBM] [Info] Start training from score 0.014499\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2640, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1652\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503625 -> initscore=0.014499\n",
      "[LightGBM] [Info] Start training from score 0.014499\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2640, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503625 -> initscore=0.014499\n",
      "[LightGBM] [Info] Start training from score 0.014499\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2640, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503625 -> initscore=0.014499\n",
      "[LightGBM] [Info] Start training from score 0.014499\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2640, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1652\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503625 -> initscore=0.014499\n",
      "[LightGBM] [Info] Start training from score 0.014499\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2640, number of negative: 2602\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1652\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503625 -> initscore=0.014499\n",
      "[LightGBM] [Info] Start training from score 0.014499\n",
      "==================================================\n",
      "üîπ LightGBM\n",
      "Accuracy per fold: [0.81989708 0.7838765  0.80617496 0.7941681  0.81099656 0.81443299\n",
      " 0.8161512  0.81443299 0.81443299 0.81786942]\n",
      "Mean Accuracy: 0.8092 (+/- 0.0110)\n",
      "==================================================\n",
      "üîπ CatBoost\n",
      "Accuracy per fold: [0.81818182 0.79759863 0.79759863 0.80960549 0.84020619 0.81443299\n",
      " 0.82646048 0.80584192 0.81271478 0.79896907]\n",
      "Mean Accuracy: 0.8122 (+/- 0.0130)\n"
     ]
    }
   ],
   "source": [
    "# 4. –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Mean Accuracy\": scores.mean(),\n",
    "        \"Std Dev\": scores.std()\n",
    "    })\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üîπ {name}\")\n",
    "    print(\"Accuracy per fold:\", scores)\n",
    "    print(f\"Mean Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40357ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Cross-Validation Results:\n",
      "       Model  Mean Accuracy   Std Dev\n",
      "1  CatBoost       0.812161  0.012951\n",
      "0  LightGBM       0.809243  0.010953\n"
     ]
    }
   ],
   "source": [
    "# 5. –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Mean Accuracy\", ascending=False)\n",
    "print(\"\\nüìä Cross-Validation Results:\\n\", results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
